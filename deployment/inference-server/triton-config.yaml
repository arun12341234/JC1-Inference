# ðŸš€ NVIDIA Triton Inference Server Configuration
backend-config:
  tensorflow:
    allow-soft-placement: true
    memory-growth: true
  tensorrt:
    force-implicit-batch: false  # ðŸ”¹ Allows dynamic batching for better efficiency
  pytorch:
    enable-torchscript: true

# ðŸ”¹ Enable dynamic batching to optimize inference latency
model-batch:
  max-batch-size: 64  # ðŸ”¹ Maximum batch size per inference request

# ðŸ”¹ GPU Configuration (Allocate H100 GPUs)
instance-groups:
  - name: "jc1-instance"
    count: 1
    kind: KIND_GPU
    gpus: [0]  # ðŸ”¹ Assigns the first available GPU (Adjust if needed)
    optimization:
      execution_accelerators:
        gpu_execution_accelerator:
          - name: "tensorrt"

# ðŸ”¹ Set model repository path (ensure PVC or mounted storage is configured)
model-repository: /models/triton-repository

# ðŸ”¹ Enable response caching for faster inference on repeated requests
cache-config:
  enable-response-cache: true
  cache-directory: /models/cache

# ðŸ”¹ Define logging level
log-level: INFO
